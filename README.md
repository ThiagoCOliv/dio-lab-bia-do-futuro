# ğŸ¤– Agente de Planejamento Financeiro â€” Projeto

Este repositÃ³rio contÃ©m um protÃ³tipo de um agente de planejamento financeiro que combina dados locais (perfil, transaÃ§Ãµes e histÃ³rico) com um modelo de linguagem (LLM) â€” no projeto atual, a integraÃ§Ã£o Ã© feita via um servidor Ollama local. O front-end Ã© uma app simples em Streamlit (`src/app.py`) que oferece uma interface de chat para perguntas financeiras contextualizadas.

O README a seguir explica como executar o projeto, dependÃªncias, arquivos de dados e pontos importantes para testar e estender o agente.

## ConteÃºdo principal

- `src/app.py` â€” AplicaÃ§Ã£o principal em Streamlit. Carrega dados da pasta `data/`, monta um contexto e envia prompts ao modelo via HTTP (configurado por `OLLAMA_URL`).
- `data/` â€” Dados mockados usados pelo agente: perfil do cliente, transaÃ§Ãµes, histÃ³rico de atendimento e catÃ¡logo de produtos.
- `docs/` â€” DocumentaÃ§Ã£o e templates (documentaÃ§Ã£o do agente, prompts, mÃ©tricas, pitch).
- `examples/` â€” ReferÃªncias e exemplos auxiliares.

## DependÃªncias

Bibliotecas usadas diretamente pelo cÃ³digo:

- streamlit
- pandas
- requests

RecomendaÃ§Ã£o: use um ambiente virtual. Para instalar rapidamente as dependÃªncias (PowerShell):

```powershell
python -m venv .venv
. .\.venv\Scripts\Activate.ps1
python -m pip install --upgrade pip
python -m pip install pandas requests streamlit
```

Se preferir travar versÃµes, crie `requirements.txt` com essas dependÃªncias e instale com `python -m pip install -r requirements.txt`.

## PrÃ©-requisitos importantes

- Servidor Ollama local rodando e acessÃ­vel em `http://localhost:11434` (ou ajuste `OLLAMA_URL` em `src/app.py`).
- Um modelo compatÃ­vel carregado no Ollama. O cÃ³digo usa por padrÃ£o `MODELO = "gemma3:4b"` â€” verifique se esse modelo ou outro equivalente estÃ¡ disponÃ­vel no seu Ollama.

Sem o Ollama ativo, o app farÃ¡ a requisiÃ§Ã£o HTTP e terÃ¡ erro ao tentar obter respostas do agente. Ã‰ possÃ­vel usar o Streamlit apenas para visualizar que os dados foram carregados, mas o chat depende do endpoint.

## Como rodar (PowerShell)

1. Abra um terminal PowerShell na raiz do repositÃ³rio.
2. (Opcional) Crie e ative um ambiente virtual:

```powershell
python -m venv .venv
. .\.venv\Scripts\Activate.ps1
```

3. Instale dependÃªncias (se ainda nÃ£o tiver instalado):

```powershell
python -m pip install pandas requests streamlit
```

4. Inicie o Ollama e carregue o modelo desejado (veja documentaÃ§Ã£o do Ollama).
5. Execute o app (a partir da pasta `src`):

```powershell
cd src
streamlit run app.py
```

ObservaÃ§Ã£o: se preferir rodar direto da raiz, use `streamlit run src/app.py`.

## Dados usados pelo agente

Os arquivos em `data/` jÃ¡ contÃªm exemplos para um cliente fictÃ­cio (Carlos Mendes). SÃ£o eles:

- `perfil_investidor.json` â€” nome, idade, renda, metas e perfil de risco.
- `transacoes.csv` â€” histÃ³rico de entradas/saÃ­das usado para projeÃ§Ãµes de fluxo de caixa.
- `historico_atendimento.csv` â€” registro de interaÃ§Ãµes anteriores que entram no contexto.
- `produtos_financeiros.json` â€” catÃ¡logo de produtos que o agente pode recomendar.

Sinta-se livre para editar ou ampliar esses arquivos para testar cenÃ¡rios diferentes.

## Estrutura do projeto (resumida)

```
dio-lab-bia-do-futuro/
â”œâ”€â”€ README.md                # este arquivo
â”œâ”€â”€ data/                    # dados mockados (perfil, transaÃ§Ãµes, produtos, histÃ³rico)
â”œâ”€â”€ docs/                    # documentaÃ§Ã£o e templates (prompts, mÃ©tricas, pitch)
â”œâ”€â”€ examples/                # exemplos e guias rÃ¡pidos
â””â”€â”€ src/
    â”œâ”€â”€ app.py               # app Streamlit que monta contexto e consulta o modelo via Ollama
    â””â”€â”€ README.md            # instruÃ§Ãµes especÃ­ficas da pasta src
```

## RecomendaÃ§Ãµes de melhorias (prÃ³ximos passos)

- Criar `src/requirements.txt` com versÃµes pinadas.
- Tratar exceÃ§Ãµes em `src/app.py` para capturar erros de conexÃ£o com o Ollama e exibir mensagens amigÃ¡veis no Streamlit.
- Adicionar testes unitÃ¡rios simples que validem a leitura dos arquivos em `data/` e a montagem do `contexto`.
- Implementar logs e/ou modo de debug para inspecionar o prompt enviado ao modelo.